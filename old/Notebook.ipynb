{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we like to introduce in the topic of Tranformer structures which use Attention."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "Given a list of tokens T, one can represent every the trough a vector.\n",
    "Therefore T can be encoded as a matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "T=[\"The\", \"bear market\", \"bull market\", \"will\", \"will not\", \"crash\", \"ralley\"]\n",
    "\n",
    "T_rep_1 = np.array([1,0,0,0,0,0,0])\n",
    "T_rep_2 = np.array([0,1,0,0,0,0,0])\n",
    "T_rep_3 = np.array([0,0,1,0,0,0,0])\n",
    "T_rep_4 = np.array([0,0,0,1,0,0,0])\n",
    "T_rep_5 = np.array([0,0,0,0,1,0,0])\n",
    "T_rep_6 = np.array([0,0,0,0,0,1,0])\n",
    "T_rep_7 = np.array([0,0,0,0,0,0,1])\n",
    "\n",
    "T_matrix = np.array([T_rep_1, T_rep_2, T_rep_3, T_rep_4, T_rep_5, T_rep_6, T_rep_7])\n",
    "print(T_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First order sequence model\n",
    "We can use a markov transition matrix M to encode how likely it is that word A comes after word B.\n",
    "In this Example we allow 2 possible sentences which are: \\\n",
    "\"The bull market will crash\"\\\n",
    "\"The bull market will not crash\"\\\n",
    "In the transition matrix here we can see the structure of our 2 sentences clearly. Almost all of the transition probabilities are zero or one. There is only one place in the Markov chain where branching happens. After \"market\" the tokens \"will\" and \"will not\". Other than that, thereâ€™s no uncertainty about which word will come next. That certainty is reflected by having mostly ones and zeros in the transition matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.5 0.5 0.  0. ]\n",
      " [0.  0.  0.  0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "M = np.array([[0,0,1,0,0,0,0], [0,0,0,0,0,0,0], [0,0,0,0.5,0.5,0,0], [0,0,0,0,0,1,0], [0,0,0,0,0,1,0], [0,0,0,0,0,0,0], [0,0,0,0,0,0,0]])\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we like to pull out the transition probabilities associated with a given token, we can use matrix multiplication with the representation of that token. So for the transition probability of the token \"market\" we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.  0.  0.5 0.5 0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "prob = np.matmul(T_rep_3, M)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Imagine we are building yet another language model. This one has to represent 4 sentences, each equally likely to occur. \n",
    "\n",
    "\n",
    "The bear market will not crash \\\n",
    "The bear market will ralley \\\n",
    "The bull market will not ralley \\\n",
    "The bull market will crash \n",
    "\n",
    "\n",
    "All sentences contain 4 tokens. Furthermore one can predict the last token, if he knows token 2 and token 3.\n",
    "We can see that only \"bear market\", \"bull market\", \"will\" and \"will not\" are important keywords for the last token.\n",
    "\n",
    "For example, whenever \"bear market\" and \"will not\" has been seen so far we know \"crash\" is the last token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
